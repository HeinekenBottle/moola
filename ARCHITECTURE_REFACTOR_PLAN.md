# Moola Architecture Refactor Plan
**Date:** 2025-10-20  
**Current Dataset:** 174 labeled samples (small dataset regime)  
**Goal:** Clean directory structure with clear data/model/encoder taxonomy

---

## ðŸŽ¯ Executive Summary

This refactor addresses three critical issues:
1. **Root directory clutter** - AI agent configs, duplicate documentation, scattered artifacts
2. **Ambiguous data taxonomy** - Unclear distinction between unlabeled/labeled, 4D/11D, raw/processed
3. **Confusing model/encoder naming** - Mixed terminology for encoders, models, weights, checkpoints

**Key Principle:** Distinguish **encoders** (reusable feature extractors) from **models** (complete architectures) from **weights** (saved checkpoints).

---

## ðŸ“‹ Part 1: Root Directory Cleanup

### Current Problems
```
/Users/jack/projects/moola/
â”œâ”€â”€ .mcp.json                          # Empty, duplicates global config
â”œâ”€â”€ .env                               # Contains GLM_API_KEY (not used by Claude Code)
â”œâ”€â”€ .claude/                           # Local Claude Code cache (OK to keep)
â”œâ”€â”€ .factory/                          # Local Factory cache (OK to keep)
â”œâ”€â”€ claude_code_zai_env.sh             # Duplicate of global config
â”œâ”€â”€ code_reviewer_agent.md             # OpenCode agent (belongs in ~/dotfiles)
â”œâ”€â”€ data_scientist_agent.md            # OpenCode agent (belongs in ~/dotfiles)
â”œâ”€â”€ ml_engineer_agent.md               # OpenCode agent (belongs in ~/dotfiles)
â”œâ”€â”€ python_pro_agent.md                # OpenCode agent (belongs in ~/dotfiles)
â”œâ”€â”€ code_review_command.md             # OpenCode command (belongs in ~/dotfiles)
â”œâ”€â”€ data_analysis_command.md           # OpenCode command (belongs in ~/dotfiles)
â”œâ”€â”€ ml_pipeline_command.md             # OpenCode command (belongs in ~/dotfiles)
â”œâ”€â”€ python_dev_command.md              # OpenCode command (belongs in ~/dotfiles)
â”œâ”€â”€ model_174_baseline.pkl             # Scattered model artifact
â”œâ”€â”€ model_174_pretrained.pkl           # Scattered model artifact
â”œâ”€â”€ feature_metadata_174.json          # Scattered metadata
â”œâ”€â”€ test_oof.npy                       # Scattered OOF predictions
â”œâ”€â”€ test_splits/                       # Duplicate of data/splits/
â”œâ”€â”€ runpod_bundle_*.tar.gz             # Build artifacts (should be in artifacts/)
â”œâ”€â”€ runpod_bundle_20251020_013740/     # Unpacked bundle (should be in artifacts/)
â”œâ”€â”€ runpod_bundle_build/               # Build directory (should be in artifacts/)
â”œâ”€â”€ runpod_results/                    # Duplicate of artifacts/runpod_results/
â”œâ”€â”€ CLAUDE_DESKTOP_ML_TRAINING_PROMPT.md  # Duplicate documentation
â”œâ”€â”€ CLEANUP_SUMMARY_2025-10-19.md      # Temporary documentation
â”œâ”€â”€ IMPLEMENTATION_11D_GUIDE.md        # Should be in docs/
â”œâ”€â”€ TRANSFER_LEARNING_PROGRESS_SUMMARY.md  # Should be in docs/
â”œâ”€â”€ WELCOME_BACK.md                    # Temporary documentation
â””â”€â”€ WORKFLOW_SSH_SCP_GUIDE.md          # Should be in docs/
```

### âœ… Files to KEEP at Root
```
/Users/jack/projects/moola/
â”œâ”€â”€ .git/                              # Version control
â”œâ”€â”€ .gitignore                         # Git ignore patterns
â”œâ”€â”€ .pre-commit-config.yaml            # Pre-commit hooks
â”œâ”€â”€ .env.example                       # Example environment variables (no secrets)
â”œâ”€â”€ .claude/                           # Local Claude Code cache (auto-generated)
â”œâ”€â”€ .factory/                          # Local Factory cache (auto-generated)
â”œâ”€â”€ .dvc/                              # DVC version control
â”œâ”€â”€ .dvcignore                         # DVC ignore patterns
â”œâ”€â”€ .venv/                             # Python virtual environment
â”œâ”€â”€ pyproject.toml                     # Python project configuration
â”œâ”€â”€ uv.lock                            # UV lock file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ requirements-runpod.txt            # RunPod-specific dependencies
â”œâ”€â”€ requirements_production.txt        # Production dependencies
â”œâ”€â”€ Makefile                           # Build automation
â”œâ”€â”€ README.md                          # Project overview
â”œâ”€â”€ CLAUDE.md                          # Claude Code context (main guide)
â”œâ”€â”€ RUNPOD_QUICK_START.md              # RunPod workflow guide
â””â”€â”€ dvc.yaml                           # DVC pipeline configuration
```

### âŒ Files to DELETE
```
# AI Agent Configs (duplicates of ~/dotfiles)
.mcp.json                              # Empty, use global config
.env                                   # Contains unused GLM_API_KEY
claude_code_zai_env.sh                 # Duplicate of global config
*_agent.md                             # 4 files - OpenCode agents (global)
*_command.md                           # 4 files - OpenCode commands (global)

# Temporary Documentation (archive or delete)
CLEANUP_SUMMARY_2025-10-19.md          # Temporary, archive if needed
WELCOME_BACK.md                        # Temporary, delete
CLAUDE_DESKTOP_ML_TRAINING_PROMPT.md   # Duplicate, delete

# Scattered Artifacts (move to artifacts/)
model_174_baseline.pkl                 # â†’ artifacts/models/supervised/
model_174_pretrained.pkl               # â†’ artifacts/models/supervised/
feature_metadata_174.json              # â†’ artifacts/metadata/
test_oof.npy                           # â†’ artifacts/oof/
test_splits/                           # â†’ DELETE (duplicate of data/splits/)
runpod_bundle_*.tar.gz                 # â†’ artifacts/runpod_bundles/
runpod_bundle_20251020_013740/         # â†’ artifacts/runpod_bundles/
runpod_bundle_build/                   # â†’ artifacts/runpod_bundles/
runpod_results/                        # â†’ artifacts/runpod_results/ (already exists)
```

### ðŸ“ Files to MOVE to docs/
```
IMPLEMENTATION_11D_GUIDE.md            # â†’ docs/guides/
TRANSFER_LEARNING_PROGRESS_SUMMARY.md # â†’ docs/progress/
WORKFLOW_SSH_SCP_GUIDE.md              # â†’ docs/workflows/ (or keep at root)
```

---

## ðŸ“‹ Part 2: Data Taxonomy Refactor

### Current Problems
- `data/processed/` contains 15+ parquet files with unclear naming
- `data/pretraining/` mixes unlabeled OHLC with features
- `data/raw/` only has unlabeled data, no labeled raw data
- Unclear which files are 4D OHLC vs 11D RelativeTransform
- No clear separation of train/val/test splits

### Proposed Data Structure
```
data/
â”œâ”€â”€ raw/                               # Raw data (never modified)
â”‚   â”œâ”€â”€ unlabeled/
â”‚   â”‚   â””â”€â”€ unlabeled_windows.parquet  # 2.2M samples, 4D OHLC
â”‚   â””â”€â”€ labeled/
â”‚       â””â”€â”€ (future: raw labeled data before cleaning)
â”‚
â”œâ”€â”€ processed/                         # Processed datasets ready for training
â”‚   â”œâ”€â”€ unlabeled/
â”‚   â”‚   â”œâ”€â”€ unlabeled_4d_ohlc.npy      # 2.2M samples, (N, 105, 4)
â”‚   â”‚   â””â”€â”€ unlabeled_11d_relative.npy # 2.2M samples, (N, 105, 11)
â”‚   â”‚
â”‚   â”œâ”€â”€ labeled/
â”‚   â”‚   â”œâ”€â”€ train_latest.parquet       # Current training set (174 samples)
â”‚   â”‚   â”œâ”€â”€ train_latest_4d.npy        # 4D OHLC version
â”‚   â”‚   â”œâ”€â”€ train_latest_11d.npy       # 11D RelativeTransform version
â”‚   â”‚   â””â”€â”€ metadata/
â”‚   â”‚       â”œâ”€â”€ feature_metadata_174.json
â”‚   â”‚       â””â”€â”€ dataset_manifest.json
â”‚   â”‚
â”‚   â””â”€â”€ archived/                      # Historical datasets
â”‚       â”œâ”€â”€ train_clean.parquet        # 98 samples (before batch 200)
â”‚       â”œâ”€â”€ train_combined_174.parquet
â”‚       â”œâ”€â”€ train_combined_175.parquet
â”‚       â”œâ”€â”€ train_combined_178.parquet
â”‚       â”œâ”€â”€ train_pivot_134.parquet
â”‚       â”œâ”€â”€ train_smote_300.parquet    # Synthetic augmentation
â”‚       â””â”€â”€ README.md                  # Explains each archived dataset
â”‚
â”œâ”€â”€ splits/                            # Train/val/test splits
â”‚   â”œâ”€â”€ fwd_chain_v3.json              # Forward chaining split config
â”‚   â”œâ”€â”€ fold_0_train.npy               # 5-fold CV splits
â”‚   â”œâ”€â”€ fold_0_val.npy
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ fold_4_val.npy
â”‚
â”œâ”€â”€ oof/                               # Out-of-fold predictions
â”‚   â”œâ”€â”€ supervised/                    # From supervised models
â”‚   â”‚   â”œâ”€â”€ simple_lstm_clean.npy
â”‚   â”‚   â”œâ”€â”€ simple_lstm_augmented.npy
â”‚   â”‚   â”œâ”€â”€ cnn_transformer_clean.npy
â”‚   â”‚   â”œâ”€â”€ logreg_clean.npy
â”‚   â”‚   â”œâ”€â”€ rf_clean.npy
â”‚   â”‚   â””â”€â”€ xgb_clean.npy
â”‚   â””â”€â”€ pretrained/                    # From pretrained models
â”‚       â””â”€â”€ (future: OOF from pretrained encoders)
â”‚
â”œâ”€â”€ batches/                           # Annotation batches
â”‚   â”œâ”€â”€ batch_200.parquet
â”‚   â”œâ”€â”€ batch_200_clean_keepers.parquet
â”‚   â”œâ”€â”€ batch_200_manifest.json
â”‚   â”œâ”€â”€ batch_201.parquet
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ README.md                      # Batch extraction workflow
â”‚
â”œâ”€â”€ corrections/                       # Human annotations & quality control
â”‚   â”œâ”€â”€ candlesticks_annotations/      # Annotation JSON files
â”‚   â”œâ”€â”€ window_blacklist.csv           # D-grade windows (permanent exclusion)
â”‚   â”œâ”€â”€ cleanlab_label_issues.csv      # Label quality analysis
â”‚   â””â”€â”€ README.md                      # Annotation workflow
â”‚
â””â”€â”€ artifacts/                         # Experiment artifacts (MOVE TO TOP-LEVEL)
    â””â”€â”€ (see Part 3)
```

### Key Changes
1. **Separate unlabeled/labeled** - Clear distinction in `processed/`
2. **Separate 4D/11D** - Explicit naming for OHLC vs RelativeTransform
3. **Archive old datasets** - Move historical datasets to `processed/archived/`
4. **Separate OOF by source** - Supervised vs pretrained models
5. **Move artifacts/** - Promote to top-level (see Part 3)

---

## ðŸ“‹ Part 3: Model/Encoder Taxonomy Refactor

### Current Problems
- `models/pretrained/bilstm_encoder.pt` - Is this an encoder or a model?
- `artifacts/models/enhanced_simple_lstm/model.pkl` - Is this pretrained or supervised?
- `artifacts/runpod_results/simple_lstm_with_pretrained_encoder.pkl` - Unclear naming
- No distinction between encoder weights, model checkpoints, and fine-tuned models

### Proposed Structure
```
artifacts/
â”œâ”€â”€ encoders/                          # Reusable feature extraction blocks
â”‚   â”œâ”€â”€ pretrained/                    # Self-supervised pretrained encoders
â”‚   â”‚   â”œâ”€â”€ bilstm_mae_4d_v1.pt        # BiLSTM Masked Autoencoder (4D OHLC)
â”‚   â”‚   â”œâ”€â”€ bilstm_mae_11d_v1.pt       # BiLSTM Masked Autoencoder (11D Relative)
â”‚   â”‚   â”œâ”€â”€ ts2vec_encoder_v1.pt       # TS2Vec contrastive encoder
â”‚   â”‚   â””â”€â”€ tstcc_encoder_v1.pt        # TSTCC contrastive encoder
â”‚   â”‚
â”‚   â””â”€â”€ supervised/                    # Encoders from supervised training
â”‚       â””â”€â”€ (future: encoder blocks extracted from trained models)
â”‚
â”œâ”€â”€ models/                            # Complete model checkpoints
â”‚   â”œâ”€â”€ supervised/                    # Supervised training (no pretraining)
â”‚   â”‚   â”œâ”€â”€ simple_lstm_baseline_174.pkl
â”‚   â”‚   â”œâ”€â”€ enhanced_simple_lstm_174.pkl
â”‚   â”‚   â”œâ”€â”€ cnn_transformer_174.pkl
â”‚   â”‚   â”œâ”€â”€ rwkv_ts_174.pkl
â”‚   â”‚   â”œâ”€â”€ logreg_174.pkl
â”‚   â”‚   â”œâ”€â”€ rf_174.pkl
â”‚   â”‚   â””â”€â”€ xgb_174.pkl
â”‚   â”‚
â”‚   â”œâ”€â”€ pretrained/                    # Fine-tuned from pretrained encoders
â”‚   â”‚   â”œâ”€â”€ simple_lstm_bilstm_mae_4d_174.pkl
â”‚   â”‚   â”œâ”€â”€ simple_lstm_bilstm_mae_11d_174.pkl
â”‚   â”‚   â”œâ”€â”€ enhanced_simple_lstm_ts2vec_174.pkl
â”‚   â”‚   â””â”€â”€ cnn_transformer_tstcc_174.pkl
â”‚   â”‚
â”‚   â””â”€â”€ ensemble/                      # Stacking ensemble models
â”‚       â”œâ”€â”€ stack_rf_meta_174.pkl      # Random Forest meta-learner
â”‚       â””â”€â”€ stack_logreg_meta_174.pkl  # Logistic Regression meta-learner
â”‚
â”œâ”€â”€ oof/                               # Out-of-fold predictions (MOVE FROM data/)
â”‚   â”œâ”€â”€ supervised/
â”‚   â”‚   â”œâ”€â”€ simple_lstm_174.npy
â”‚   â”‚   â”œâ”€â”€ enhanced_simple_lstm_174.npy
â”‚   â”‚   â”œâ”€â”€ cnn_transformer_174.npy
â”‚   â”‚   â”œâ”€â”€ logreg_174.npy
â”‚   â”‚   â”œâ”€â”€ rf_174.npy
â”‚   â”‚   â””â”€â”€ xgb_174.npy
â”‚   â”‚
â”‚   â””â”€â”€ pretrained/
â”‚       â”œâ”€â”€ simple_lstm_bilstm_mae_4d_174.npy
â”‚       â””â”€â”€ simple_lstm_bilstm_mae_11d_174.npy
â”‚
â”œâ”€â”€ metadata/                          # Experiment metadata
â”‚   â”œâ”€â”€ feature_metadata_174.json
â”‚   â”œâ”€â”€ dataset_manifest.json
â”‚   â””â”€â”€ experiment_registry.json
â”‚
â”œâ”€â”€ runpod_bundles/                    # RunPod deployment bundles
â”‚   â”œâ”€â”€ runpod_bundle_20251020_013740.tar.gz
â”‚   â”œâ”€â”€ runpod_bundle_20251020_013740/
â”‚   â””â”€â”€ runpod_bundle_build/
â”‚
â””â”€â”€ runpod_results/                    # Results from RunPod training
    â”œâ”€â”€ phase2_results.csv
    â””â”€â”€ oof/
```

### Naming Convention
```
Format: {architecture}_{pretraining}_{features}_{dataset_size}.{ext}

Examples:
- bilstm_mae_4d_v1.pt              # BiLSTM encoder, MAE pretrained, 4D OHLC, version 1
- simple_lstm_baseline_174.pkl     # SimpleLSTM, no pretraining, 174 samples
- simple_lstm_bilstm_mae_11d_174.pkl  # SimpleLSTM, BiLSTM MAE encoder, 11D, 174 samples
- enhanced_simple_lstm_ts2vec_174.pkl # EnhancedSimpleLSTM, TS2Vec encoder, 174 samples
```

---

## ðŸ“‹ Part 4: Migration Plan

### Phase 1: Root Cleanup (Low Risk)
```bash
# 1. Delete AI agent configs (backed up in ~/dotfiles)
rm .mcp.json .env claude_code_zai_env.sh
rm *_agent.md *_command.md

# 2. Delete temporary documentation
rm CLEANUP_SUMMARY_2025-10-19.md WELCOME_BACK.md
rm CLAUDE_DESKTOP_ML_TRAINING_PROMPT.md

# 3. Create .env.example (no secrets)
echo "# Project-specific environment variables" > .env.example
echo "# Global API keys are in ~/dotfiles/.env" >> .env.example
```

### Phase 2: Move Scattered Artifacts (Medium Risk)
```bash
# 1. Create new artifact directories
mkdir -p artifacts/models/supervised
mkdir -p artifacts/metadata
mkdir -p artifacts/runpod_bundles

# 2. Move scattered model files
mv model_174_baseline.pkl artifacts/models/supervised/simple_lstm_baseline_174.pkl
mv model_174_pretrained.pkl artifacts/models/supervised/simple_lstm_pretrained_174.pkl
mv feature_metadata_174.json artifacts/metadata/

# 3. Move OOF predictions
mv test_oof.npy artifacts/oof/simple_lstm_174.npy

# 4. Move RunPod bundles
mv runpod_bundle_*.tar.gz artifacts/runpod_bundles/
mv runpod_bundle_20251020_013740 artifacts/runpod_bundles/
mv runpod_bundle_build artifacts/runpod_bundles/

# 5. Delete duplicate directories
rm -rf test_splits/  # Duplicate of data/splits/
rm -rf runpod_results/  # Duplicate of artifacts/runpod_results/
```

### Phase 3: Data Taxonomy Refactor (High Risk - Requires Testing)
```bash
# 1. Create new data structure
mkdir -p data/raw/unlabeled data/raw/labeled
mkdir -p data/processed/unlabeled data/processed/labeled/metadata
mkdir -p data/processed/archived
mkdir -p data/oof/supervised data/oof/pretrained

# 2. Move unlabeled data
# (Already in correct location: data/raw/unlabeled_windows.parquet)

# 3. Rename current training dataset
cp data/processed/train_latest.parquet data/processed/labeled/train_latest.parquet

# 4. Archive old datasets
mv data/processed/train_clean.parquet data/processed/archived/
mv data/processed/train_combined_*.parquet data/processed/archived/
mv data/processed/train_pivot_134.parquet data/processed/archived/
mv data/processed/train_smote_300.parquet data/processed/archived/

# 5. Move OOF predictions
mv data/oof/*.npy data/oof/supervised/

# 6. Create README files
# (Document each archived dataset)
```

### Phase 4: Model/Encoder Taxonomy Refactor (High Risk - Requires Code Changes)
```bash
# 1. Create new encoder/model structure
mkdir -p artifacts/encoders/pretrained artifacts/encoders/supervised
mkdir -p artifacts/models/supervised artifacts/models/pretrained artifacts/models/ensemble

# 2. Rename existing encoder
mv models/pretrained/bilstm_encoder.pt artifacts/encoders/pretrained/bilstm_mae_4d_v1.pt

# 3. Move existing models
mv artifacts/models/enhanced_simple_lstm/model.pkl artifacts/models/supervised/enhanced_simple_lstm_174.pkl
mv artifacts/models/logreg/model.pkl artifacts/models/supervised/logreg_174.pkl

# 4. Update code references
# (Requires updating paths in src/moola/models/pretrained_utils.py, etc.)
```

---

## ðŸ“‹ Part 5: Code Changes Required

### Files to Update
1. **src/moola/paths.py** - Update all artifact paths
2. **src/moola/models/pretrained_utils.py** - Update encoder loading paths
3. **src/moola/pretraining/masked_lstm_pretrain.py** - Update save paths
4. **src/moola/cli.py** - Update default paths
5. **scripts/*.py** - Update all hardcoded paths
6. **tests/*.py** - Update test fixture paths

### Example Path Updates
```python
# OLD
ENCODER_PATH = "models/pretrained/bilstm_encoder.pt"
MODEL_PATH = "artifacts/models/enhanced_simple_lstm/model.pkl"

# NEW
ENCODER_PATH = "artifacts/encoders/pretrained/bilstm_mae_4d_v1.pt"
MODEL_PATH = "artifacts/models/supervised/enhanced_simple_lstm_174.pkl"
```

---

## ðŸ“‹ Part 6: Documentation Updates

### Files to Create
1. **data/processed/archived/README.md** - Explain each archived dataset
2. **artifacts/encoders/README.md** - Explain encoder taxonomy
3. **artifacts/models/README.md** - Explain model taxonomy
4. **MIGRATION_GUIDE.md** - Document migration process

### Files to Update
1. **CLAUDE.md** - Update directory structure section
2. **README.md** - Update quick start paths
3. **docs/ARCHITECTURE.md** - Update architecture diagrams

---

## âœ… Success Criteria

1. **Root directory** - Only essential project files (â‰¤15 files)
2. **Data taxonomy** - Clear separation of unlabeled/labeled, 4D/11D, raw/processed
3. **Model taxonomy** - Clear distinction between encoders, models, weights
4. **No duplicates** - No duplicate configs, artifacts, or documentation
5. **Tests pass** - All existing tests pass after migration
6. **Documentation** - All paths updated in docs and CLAUDE.md

---

## ðŸš¨ Risks & Mitigation

### High Risk
- **Breaking existing code** - Mitigation: Update all path references before moving files
- **Losing experiment history** - Mitigation: Archive old datasets with README
- **RunPod workflow breaks** - Mitigation: Test bundle creation after refactor

### Medium Risk
- **Git history confusion** - Mitigation: Use `git mv` for tracked files
- **Broken symlinks** - Mitigation: Check for symlinks before moving

### Low Risk
- **Documentation out of sync** - Mitigation: Update docs in same PR as refactor

---

## ðŸ“… Recommended Execution Order

1. **Phase 1: Root Cleanup** (30 min, low risk)
2. **Phase 5: Code Changes** (2 hours, prepare for migration)
3. **Phase 2: Move Scattered Artifacts** (30 min, medium risk)
4. **Phase 3: Data Taxonomy Refactor** (1 hour, high risk - test thoroughly)
5. **Phase 4: Model/Encoder Taxonomy Refactor** (1 hour, high risk - test thoroughly)
6. **Phase 6: Documentation Updates** (1 hour, low risk)

**Total Estimated Time:** 6 hours

---

## ðŸŽ¯ Next Steps

1. **Review this plan** - Confirm approach with user
2. **Create backup branch** - `git checkout -b refactor/architecture-cleanup`
3. **Execute Phase 1** - Low-risk root cleanup
4. **Test after each phase** - Run tests, verify paths
5. **Create PR** - Review changes before merging to main

