metric,soft_span_loss,crf,winner,notes
"Training Epochs",50,50,Tie,"Both ran 50 epochs"
"Validation Loss (Start)",17.6855,66.9362,CRF,"CRF starts higher due to NLL scale"
"Validation Loss (Final)",13.5232,38.7753,Tie,"Both converged"
"Loss Reduction %",23.5%,42.1%,CRF,"But different scales - see normalized"
"Loss Reduction (Normalized)",23.5%,11.1%,Soft,"Accounting for 3.8x scale difference"
"Loss Scale Range","3-17","11-67",Soft,"Smaller scale = better gradient flow"
"Convergence Pattern","Smooth","Aggressive",Soft,"Steady improvement vs steep drops"
"Overfitting","Minor (epochs 44-50)","Minimal",CRF,"CRF more stable late"
"Pointer Weight %",46.7%,55.5%,Soft,"Soft more balanced"
"Span Weight %",24.8%,10.8%,Soft,"Soft keeps span learning strong"
"Classification Weight %",19.6%,23.8%,CRF,"Similar"
"Countdown Weight %",8.8%,9.9%,Tie,"Similar"
"Span Uncertainty (Ïƒ_span)",0.7536,1.2437,Soft,"Lower = higher confidence in span learning"
"In-Span Mean Probability",0.090,0.101,CRF,"Marginal difference"
"Out-of-Span Mean Probability",0.090,0.092,CRF,"Marginal difference"
"Probability Separation",0.000,0.009,CRF,"Neither good - features likely issue"
"Inference Complexity","Threshold","Viterbi",Soft,"Simple beats complex in production"
"Probability Calibration","Weak","Weak","Tie","Both need more training"
"Task Balance","Better","Imbalanced",Soft,"71.5% on ptr+span vs 66.3%"
"Production Readiness","High","Medium",Soft,"Simpler, more debuggable"
"Recommendation","Deploy","Research",Soft,"Use soft span for production"
