name: LSTM Optimization Pipeline

on:
  workflow_dispatch:
    inputs:
      phase:
        description: 'Which phase to run'
        required: true
        type: choice
        options:
          - 'all'
          - 'phase1_time_warp_ablation'
          - 'phase2_architecture_search'
          - 'phase3_depth_search'
          - 'promote_best_model'
      run_on_cloud:
        description: 'Use cloud GPU worker (AWS g5.xlarge)'
        required: false
        type: boolean
        default: false
      mlflow_experiment:
        description: 'MLflow experiment name'
        required: false
        type: string
        default: 'lstm-optimization-2025'
  schedule:
    # Run weekly on Sunday at 2 AM UTC
    - cron: '0 2 * * 0'

env:
  PYTHON_VERSION: '3.10'
  MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI || 'file:///tmp/mlruns' }}
  AWS_REGION: us-east-1
  EXPERIMENT_TIMEOUT: 90  # minutes per experiment

jobs:
  # ============================================================================
  # Phase 1: Time Warp Sigma Ablation (4 experiments)
  # ============================================================================
  phase1_time_warp_ablation:
    if: github.event.inputs.phase == 'all' || github.event.inputs.phase == 'phase1_time_warp_ablation'
    runs-on: ${{ github.event.inputs.run_on_cloud == 'true' && 'ubuntu-latest' || 'self-hosted' }}
    timeout-minutes: 360  # 6 hours for 4 experiments
    strategy:
      fail-fast: false  # Continue other experiments even if one fails
      matrix:
        time_warp_sigma: [0.10, 0.12, 0.15, 0.20]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for git-based versioning

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install CUDA toolkit (for GPU)
        if: runner.os == 'Linux'
        uses: Jimver/cuda-toolkit@v0.2.14
        with:
          cuda: '11.8.0'
          method: 'network'

      - name: Install dependencies
        run: |
          pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          pip install mlflow prometheus-client slack-sdk

      - name: Verify GPU availability
        if: github.event.inputs.run_on_cloud == 'false'
        run: |
          python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'Device count: {torch.cuda.device_count()}')"
          nvidia-smi || echo "nvidia-smi not available"

      - name: Generate unlabeled data with time warp
        run: |
          python scripts/generate_unlabeled_data.py \
            --output data/raw/unlabeled_windows_tw_${{ matrix.time_warp_sigma }}.parquet \
            --time-warp-sigma ${{ matrix.time_warp_sigma }} \
            --num-samples 10000 \
            --seed 1337

      - name: Pre-train Masked LSTM encoder
        timeout-minutes: ${{ fromJson(env.EXPERIMENT_TIMEOUT) }}
        run: |
          python scripts/pretrain_masked_lstm.py \
            --unlabeled-data data/raw/unlabeled_windows_tw_${{ matrix.time_warp_sigma }}.parquet \
            --output-dir data/artifacts/pretrained/masked_lstm_tw_${{ matrix.time_warp_sigma }} \
            --epochs 50 \
            --batch-size 256 \
            --learning-rate 0.001 \
            --device cuda \
            --mlflow-experiment ${{ github.event.inputs.mlflow_experiment || 'lstm-optimization-2025' }} \
            --mlflow-run-name phase1_tw_${{ matrix.time_warp_sigma }}

      - name: Fine-tune SimpleLSTM
        timeout-minutes: ${{ fromJson(env.EXPERIMENT_TIMEOUT) }}
        run: |
          python -m moola.cli oof \
            --model simple_lstm \
            --device cuda \
            --seed 1337 \
            --load-pretrained-encoder data/artifacts/pretrained/masked_lstm_tw_${{ matrix.time_warp_sigma }}/encoder.pt \
            --mlflow-tracking \
            --mlflow-experiment ${{ github.event.inputs.mlflow_experiment || 'lstm-optimization-2025' }}

      - name: Upload artifacts to S3
        if: always()
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          aws s3 sync data/artifacts/pretrained/masked_lstm_tw_${{ matrix.time_warp_sigma }} \
            s3://moola-lstm-experiments/phase1/tw_${{ matrix.time_warp_sigma }}/ \
            --region ${{ env.AWS_REGION }}

          aws s3 cp data/artifacts/oof/simple_lstm_oof.npy \
            s3://moola-lstm-experiments/phase1/tw_${{ matrix.time_warp_sigma }}/simple_lstm_oof.npy \
            --region ${{ env.AWS_REGION }}

      - name: Export metrics to Prometheus
        if: always()
        run: |
          python scripts/export_prometheus_metrics.py \
            --experiment-name phase1_tw_${{ matrix.time_warp_sigma }} \
            --pushgateway-url ${{ secrets.PROMETHEUS_PUSHGATEWAY_URL || 'http://localhost:9091' }}

      - name: Cleanup artifacts (save space)
        if: always()
        run: |
          rm -rf data/raw/unlabeled_windows_tw_${{ matrix.time_warp_sigma }}.parquet
          rm -rf data/artifacts/pretrained/masked_lstm_tw_${{ matrix.time_warp_sigma }}

  # ============================================================================
  # Phase 2: Architecture Search (8 experiments - requires Phase 1 winner)
  # ============================================================================
  select_phase1_winner:
    if: (github.event.inputs.phase == 'all' || github.event.inputs.phase == 'phase2_architecture_search') && always()
    needs: phase1_time_warp_ablation
    runs-on: ubuntu-latest
    outputs:
      winner_sigma: ${{ steps.select.outputs.winner_sigma }}
      winner_accuracy: ${{ steps.select.outputs.winner_accuracy }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install MLflow
        run: pip install mlflow pandas

      - name: Select Phase 1 winner
        id: select
        env:
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
        run: |
          python scripts/select_phase_winner.py \
            --phase 1 \
            --experiment-name ${{ github.event.inputs.mlflow_experiment || 'lstm-optimization-2025' }} \
            --output-file phase1_winner.json

          # Extract winner details
          WINNER_SIGMA=$(jq -r '.time_warp_sigma' phase1_winner.json)
          WINNER_ACC=$(jq -r '.accuracy' phase1_winner.json)

          echo "winner_sigma=$WINNER_SIGMA" >> $GITHUB_OUTPUT
          echo "winner_accuracy=$WINNER_ACC" >> $GITHUB_OUTPUT

          echo "Phase 1 Winner: time_warp_sigma=$WINNER_SIGMA (accuracy=$WINNER_ACC)"

  phase2_architecture_search:
    if: github.event.inputs.phase == 'all' || github.event.inputs.phase == 'phase2_architecture_search'
    needs: select_phase1_winner
    runs-on: ${{ github.event.inputs.run_on_cloud == 'true' && 'ubuntu-latest' || 'self-hosted' }}
    timeout-minutes: 720  # 12 hours for 8 experiments
    strategy:
      fail-fast: false
      matrix:
        hidden_size: [64, 128]
        num_heads: [4, 8]
        num_layers: [1, 2]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install CUDA toolkit
        if: runner.os == 'Linux'
        uses: Jimver/cuda-toolkit@v0.2.14
        with:
          cuda: '11.8.0'
          method: 'network'

      - name: Install dependencies
        run: |
          pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          pip install mlflow prometheus-client slack-sdk

      - name: Download Phase 1 winner encoder
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          mkdir -p data/artifacts/pretrained/phase1_winner
          aws s3 sync s3://moola-lstm-experiments/phase1/tw_${{ needs.select_phase1_winner.outputs.winner_sigma }}/ \
            data/artifacts/pretrained/phase1_winner/ \
            --region ${{ env.AWS_REGION }}

      - name: Train SimpleLSTM with architecture variant
        timeout-minutes: ${{ fromJson(env.EXPERIMENT_TIMEOUT) }}
        run: |
          python -m moola.cli oof \
            --model simple_lstm \
            --device cuda \
            --seed 1337 \
            --hidden-size ${{ matrix.hidden_size }} \
            --num-layers ${{ matrix.num_layers }} \
            --num-heads ${{ matrix.num_heads }} \
            --load-pretrained-encoder data/artifacts/pretrained/phase1_winner/encoder.pt \
            --mlflow-tracking \
            --mlflow-experiment ${{ github.event.inputs.mlflow_experiment || 'lstm-optimization-2025' }}

      - name: Upload artifacts to S3
        if: always()
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          aws s3 cp data/artifacts/oof/simple_lstm_oof.npy \
            s3://moola-lstm-experiments/phase2/h${{ matrix.hidden_size }}_nh${{ matrix.num_heads }}_l${{ matrix.num_layers }}/simple_lstm_oof.npy \
            --region ${{ env.AWS_REGION }}

  # ============================================================================
  # Phase 3: Pre-training Depth Search (1 experiment with best config)
  # ============================================================================
  select_phase2_winner:
    if: (github.event.inputs.phase == 'all' || github.event.inputs.phase == 'phase3_depth_search') && always()
    needs: phase2_architecture_search
    runs-on: ubuntu-latest
    outputs:
      winner_config: ${{ steps.select.outputs.winner_config }}
      winner_accuracy: ${{ steps.select.outputs.winner_accuracy }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install MLflow
        run: pip install mlflow pandas

      - name: Select Phase 2 winner
        id: select
        env:
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
        run: |
          python scripts/select_phase_winner.py \
            --phase 2 \
            --experiment-name ${{ github.event.inputs.mlflow_experiment || 'lstm-optimization-2025' }} \
            --output-file phase2_winner.json

          # Extract winner config as JSON string
          WINNER_CONFIG=$(jq -c '.' phase2_winner.json)
          WINNER_ACC=$(jq -r '.accuracy' phase2_winner.json)

          echo "winner_config=$WINNER_CONFIG" >> $GITHUB_OUTPUT
          echo "winner_accuracy=$WINNER_ACC" >> $GITHUB_OUTPUT

  phase3_depth_search:
    if: github.event.inputs.phase == 'all' || github.event.inputs.phase == 'phase3_depth_search'
    needs: [select_phase1_winner, select_phase2_winner]
    runs-on: ${{ github.event.inputs.run_on_cloud == 'true' && 'ubuntu-latest' || 'self-hosted' }}
    timeout-minutes: 360  # 6 hours for depth experiments
    strategy:
      fail-fast: false
      matrix:
        pretrain_epochs: [50, 75, 100]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install CUDA toolkit
        if: runner.os == 'Linux'
        uses: Jimver/cuda-toolkit@v0.2.14
        with:
          cuda: '11.8.0'
          method: 'network'

      - name: Install dependencies
        run: |
          pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          pip install mlflow prometheus-client slack-sdk

      - name: Generate unlabeled data (Phase 1 winner config)
        run: |
          python scripts/generate_unlabeled_data.py \
            --output data/raw/unlabeled_windows_phase3.parquet \
            --time-warp-sigma ${{ needs.select_phase1_winner.outputs.winner_sigma }} \
            --num-samples 10000 \
            --seed 1337

      - name: Pre-train Masked LSTM with depth
        timeout-minutes: 180
        run: |
          python scripts/pretrain_masked_lstm.py \
            --unlabeled-data data/raw/unlabeled_windows_phase3.parquet \
            --output-dir data/artifacts/pretrained/masked_lstm_phase3_e${{ matrix.pretrain_epochs }} \
            --epochs ${{ matrix.pretrain_epochs }} \
            --batch-size 256 \
            --learning-rate 0.001 \
            --device cuda \
            --mlflow-experiment ${{ github.event.inputs.mlflow_experiment || 'lstm-optimization-2025' }} \
            --mlflow-run-name phase3_pretrain_e${{ matrix.pretrain_epochs }}

      - name: Fine-tune SimpleLSTM (Phase 2 winner architecture)
        timeout-minutes: ${{ fromJson(env.EXPERIMENT_TIMEOUT) }}
        env:
          PHASE2_CONFIG: ${{ needs.select_phase2_winner.outputs.winner_config }}
        run: |
          # Parse phase 2 winner config
          HIDDEN_SIZE=$(echo $PHASE2_CONFIG | jq -r '.hidden_size')
          NUM_HEADS=$(echo $PHASE2_CONFIG | jq -r '.num_heads')
          NUM_LAYERS=$(echo $PHASE2_CONFIG | jq -r '.num_layers')

          python -m moola.cli oof \
            --model simple_lstm \
            --device cuda \
            --seed 1337 \
            --hidden-size $HIDDEN_SIZE \
            --num-layers $NUM_LAYERS \
            --num-heads $NUM_HEADS \
            --load-pretrained-encoder data/artifacts/pretrained/masked_lstm_phase3_e${{ matrix.pretrain_epochs }}/encoder.pt \
            --mlflow-tracking \
            --mlflow-experiment ${{ github.event.inputs.mlflow_experiment || 'lstm-optimization-2025' }}

      - name: Upload artifacts to S3
        if: always()
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          aws s3 sync data/artifacts/pretrained/masked_lstm_phase3_e${{ matrix.pretrain_epochs }} \
            s3://moola-lstm-experiments/phase3/pretrain_e${{ matrix.pretrain_epochs }}/ \
            --region ${{ env.AWS_REGION }}

  # ============================================================================
  # Final: Promote Best Model
  # ============================================================================
  promote_best_model:
    if: always() && (github.event.inputs.phase == 'all' || github.event.inputs.phase == 'promote_best_model')
    needs: [phase1_time_warp_ablation, phase2_architecture_search, phase3_depth_search]
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install mlflow pandas slack-sdk

      - name: Compare all experiments and select best
        id: select_best
        env:
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
        run: |
          python scripts/select_best_model.py \
            --experiment-name ${{ github.event.inputs.mlflow_experiment || 'lstm-optimization-2025' }} \
            --output-report comparison_report.md \
            --min-class1-accuracy 0.30

          # Extract best model details
          BEST_RUN_ID=$(cat best_model.json | jq -r '.run_id')
          BEST_ACCURACY=$(cat best_model.json | jq -r '.accuracy')
          BEST_CLASS1_ACC=$(cat best_model.json | jq -r '.class_1_accuracy')

          echo "run_id=$BEST_RUN_ID" >> $GITHUB_OUTPUT
          echo "accuracy=$BEST_ACCURACY" >> $GITHUB_OUTPUT
          echo "class_1_accuracy=$BEST_CLASS1_ACC" >> $GITHUB_OUTPUT

      - name: Tag best model in MLflow
        env:
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
        run: |
          python -c "
          import mlflow
          client = mlflow.MlflowClient()

          # Tag run as production candidate
          client.set_tag('${{ steps.select_best.outputs.run_id }}', 'promotion_status', 'production_candidate')
          client.set_tag('${{ steps.select_best.outputs.run_id }}', 'promoted_at', '$(date -u +%Y-%m-%dT%H:%M:%SZ)')
          client.set_tag('${{ steps.select_best.outputs.run_id }}', 'promoted_by', 'github-actions')

          print('Tagged run ${{ steps.select_best.outputs.run_id }} as production_candidate')
          "

      - name: Upload comparison report
        uses: actions/upload-artifact@v4
        with:
          name: comparison-report
          path: comparison_report.md
          retention-days: 90

      - name: Send Slack notification
        if: always()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          python scripts/send_slack_notification.py \
            --webhook-url "$SLACK_WEBHOOK_URL" \
            --channel "#ml-experiments" \
            --title "LSTM Optimization Pipeline Complete" \
            --message "Best model: accuracy=${{ steps.select_best.outputs.accuracy }}, class_1_accuracy=${{ steps.select_best.outputs.class_1_accuracy }}" \
            --report comparison_report.md

  # ============================================================================
  # Cleanup: Terminate cloud resources
  # ============================================================================
  cleanup:
    if: always() && github.event.inputs.run_on_cloud == 'true'
    needs: [phase1_time_warp_ablation, phase2_architecture_search, phase3_depth_search, promote_best_model]
    runs-on: ubuntu-latest

    steps:
      - name: Terminate cloud GPU instances
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          # Get instance IDs from tags
          INSTANCE_IDS=$(aws ec2 describe-instances \
            --region ${{ env.AWS_REGION }} \
            --filters "Name=tag:Project,Values=moola-lstm-optimization" "Name=instance-state-name,Values=running" \
            --query "Reservations[].Instances[].InstanceId" \
            --output text)

          if [ -n "$INSTANCE_IDS" ]; then
            echo "Terminating instances: $INSTANCE_IDS"
            aws ec2 terminate-instances --region ${{ env.AWS_REGION }} --instance-ids $INSTANCE_IDS
          else
            echo "No running instances found"
          fi
