# Opal Model Configuration
# moola-preenc-ad-m-v1.0 // codename: Opal
# Pre-trained encoder with adaptive fine-tuning

model:
  name: opal
  base: jade  # Inherits from Jade
  
  # Transfer learning
  pretrained_encoder_path: artifacts/encoders/pretrained/bilstm_mae_11d_v1.pt
  freeze_encoder: false
  unfreeze_encoder_after: 10  # Adaptive fine-tuning
  
  # Full model fine-tuning
  architecture:
    hidden_size: 128
    num_layers: 2
    dense_dropout: 0.5

# Inherits augmentation, uncertainty, gates from Jade

