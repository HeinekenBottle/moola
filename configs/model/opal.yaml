# Opal Model Configuration
# moola-preenc-ad-m-v1.0 // codename: Opal
# Pre-trained encoder with adaptive fine-tuning

model:
  name: opal
  base: jade  # Inherits from Jade
  
  # Transfer learning
  pretrained_encoder_path: artifacts/encoders/pretrained/bilstm_mae_11d_v1.pt
  freeze_encoder: false
  unfreeze_encoder_after: 10  # Adaptive fine-tuning
  
  # Full model fine-tuning
  architecture:
    hidden_size: 128
    num_layers: 2
    input_dropout: 0.25  # Stones: 0.2-0.3
    recurrent_dropout: 0.65  # Stones: 0.6-0.7 (inter-layer)
    dense_dropout: 0.5  # Stones: 0.4-0.5

# Inherits augmentation, uncertainty, gates from Jade

