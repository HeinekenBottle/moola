{
  "phase": 4,
  "description": "Learning rate scheduling and early stopping improvements",
  "motivation": "Adaptive learning rate reduction helps models escape plateaus and achieve better final performance by fine-tuning with progressively smaller learning rates",

  "lr_scheduler": {
    "enabled": true,
    "type": "ReduceLROnPlateau",
    "mode": "min",
    "monitor_metric": "validation_loss",
    "factor": 0.5,
    "patience": 10,
    "threshold": 0.001,
    "threshold_mode": "rel",
    "cooldown": 0,
    "min_lr": 1e-6,
    "verbose": true
  },

  "early_stopping": {
    "patience": 20,
    "monitor": "val_loss",
    "mode": "min",
    "save_best_checkpoint": true,
    "checkpoint_dir": "artifacts/models/supervised/checkpoints"
  },

  "expected_benefits": {
    "training_stability": "Automatic LR reduction when validation loss plateaus prevents wasted epochs",
    "convergence": "Better final performance by fine-tuning with lower LR in later stages",
    "efficiency": "Early stopping prevents overfitting and saves training time",
    "diagnostics": "Enhanced logging shows LR reductions and patience tracking for better debugging"
  },

  "cli_usage": {
    "default_enabled": "python3 -m moola.cli train --model enhanced_simple_lstm --split data/splits/temporal_split_174.json --device cuda",
    "no_scheduler": "python3 -m moola.cli train --model enhanced_simple_lstm --no-use-lr-scheduler --split data/splits/temporal_split_174.json --device cuda",
    "custom_params": "python3 -m moola.cli train --model enhanced_simple_lstm --scheduler-factor 0.3 --scheduler-patience 15 --split data/splits/temporal_split_174.json --device cuda",
    "with_checkpoints": "python3 -m moola.cli train --model enhanced_simple_lstm --save-checkpoints --split data/splits/temporal_split_174.json --device cuda",
    "aggressive_reduction": "python3 -m moola.cli train --model enhanced_simple_lstm --scheduler-factor 0.2 --scheduler-patience 5 --split data/splits/temporal_split_174.json --device cuda"
  },

  "typical_training_progression": {
    "epoch_0_10": {
      "learning_rate": "3e-4",
      "expected_behavior": "Initial training phase, model learning basic patterns"
    },
    "epoch_10_20": {
      "learning_rate": "3e-4 -> potentially 1.5e-4",
      "expected_behavior": "If plateau detected after 10 epochs, LR reduced by 0.5x"
    },
    "epoch_20_30": {
      "learning_rate": "1.5e-4 -> potentially 7.5e-5",
      "expected_behavior": "Further refinement with reduced LR if needed"
    },
    "epoch_30_40": {
      "learning_rate": "7.5e-5 -> potentially 3.75e-5",
      "expected_behavior": "Fine-tuning phase with very small learning rate"
    }
  },

  "hyperparameter_tuning": {
    "scheduler_factor": {
      "conservative": 0.5,
      "moderate": 0.3,
      "aggressive": 0.2,
      "recommendation": "Start with 0.5 (conservative) for small datasets like Moola (174 samples)"
    },
    "scheduler_patience": {
      "low": 5,
      "medium": 10,
      "high": 15,
      "recommendation": "Use 10 epochs for 174-sample dataset (allows meaningful convergence check)"
    },
    "scheduler_min_lr": {
      "typical": 1e-6,
      "very_small": 1e-7,
      "recommendation": "1e-6 is sufficient for most cases"
    }
  },

  "checkpoint_management": {
    "when_enabled": "Saves best model checkpoint whenever validation loss improves",
    "location": "artifacts/models/supervised/checkpoints/best_checkpoint.pt",
    "contents": [
      "epoch",
      "model_state_dict",
      "optimizer_state_dict",
      "scheduler_state_dict",
      "best_val_loss",
      "train_loss",
      "val_accuracy"
    ],
    "restoration": "Model can be loaded from checkpoint to recover best weights after early stopping"
  },

  "logging_enhancements": {
    "lr_tracking": "Current learning rate logged after each validation phase",
    "improvement_tracking": "Shows improvement magnitude when new best validation loss achieved",
    "patience_counter": "Shows progress toward early stopping threshold",
    "final_summary": "Reports best epoch, best validation loss, and final learning rate on early stopping"
  },

  "integration_with_phases": {
    "phase_1": "Gradient clipping + layer-specific weight decay",
    "phase_2": "Latent mixup + magnitude warping augmentation",
    "phase_3": "MC Dropout + temperature scaling calibration",
    "phase_4": "Learning rate scheduling + enhanced early stopping (THIS PHASE)",
    "combined_effect": "All phases work together for robust training on small datasets"
  },

  "troubleshooting": {
    "lr_reduces_too_fast": "Increase scheduler_patience (e.g., 15 instead of 10)",
    "lr_never_reduces": "Decrease scheduler_patience or increase threshold",
    "training_plateaus_early": "Check if min_lr is too high, try 1e-7 instead of 1e-6",
    "early_stopping_too_soon": "Increase early_stopping_patience in model init (default: 20)"
  }
}
