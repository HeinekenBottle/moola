"""Out-of-fold (OOF) prediction generation for ensemble stacking.

OOF predictions are generated by K-fold cross-validation:
1. Split data into K folds using shared split manifests
2. For each fold, train model on K-1 folds
3. Predict probabilities on the held-out fold
4. Concatenate all fold predictions into [N, C] matrix

This ensures no data leakage when training the meta-learner.
"""

from pathlib import Path

import numpy as np
from loguru import logger
# SMOTE removed per Phase 1c - use controlled augmentation with KS p-value validation instead
# from imblearn.over_sampling import SMOTE

try:
    import mlflow
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False
    logger.warning("MLflow not available. Install with: pip install mlflow")

from ..models import get_model
from ..utils.splits import get_or_create_splits


def generate_oof(
    X: np.ndarray,
    y: np.ndarray,
    model_name: str,
    seed: int,
    k: int,
    splits_dir: Path,
    output_path: Path,
    device: str = "cpu",
    expansion_start: np.ndarray = None,
    expansion_end: np.ndarray = None,
    apply_smote: bool = False,  # DEPRECATED: Use controlled augmentation instead
    smote_target_count: int = 150,  # DEPRECATED: Use controlled augmentation instead
    mlflow_tracking: bool = False,
    mlflow_experiment: str = "moola-oof",
    **model_kwargs,
) -> np.ndarray:
    """Generate out-of-fold predictions for a given model.

    Args:
        X: Feature matrix of shape [N, D]
        y: Target labels of shape [N]
        model_name: Model name (logreg, rf, xgb, rwkv_ts, simple_lstm, cnn_transformer)
        seed: Random seed for reproducibility
        k: Number of folds
        splits_dir: Directory containing split manifests
        output_path: Path to save OOF predictions (.npy file)
        device: Device for training deep learning models ('cpu' or 'cuda')
        expansion_start: Optional expansion start indices of shape [N]
        expansion_end: Optional expansion end indices of shape [N]
        apply_smote: Whether to apply SMOTE augmentation per-fold (default: False)
        smote_target_count: Target samples per class for SMOTE (default: 150)
        mlflow_tracking: Whether to log metrics to MLflow (default: False)
        mlflow_experiment: MLflow experiment name (default: "moola-oof")
        **model_kwargs: Additional model hyperparameters

    Returns:
        OOF predictions of shape [N, C] where C is number of classes

    Side Effects:
        - Creates split manifests in splits_dir if they don't exist
        - Saves OOF predictions to output_path
        - Creates .build.lock during write operations
        - Logs metrics to MLflow if mlflow_tracking=True

    Note:
        If apply_smote=True, SMOTE is applied PER-FOLD to avoid data leakage.
        Only training folds are augmented; validation folds remain original samples.
    """
    logger.info(f"Generating OOF predictions | model={model_name} seed={seed} k={k}")

    # Get or create shared splits
    splits = get_or_create_splits(X, y, seed=seed, k=k, splits_dir=splits_dir)

    # Get number of classes from unique labels
    n_samples = len(y)
    n_classes = len(np.unique(y))

    # Initialize OOF prediction matrix [N, C]
    oof_predictions = np.zeros((n_samples, n_classes), dtype=np.float64)

    logger.info(f"OOF matrix shape: {oof_predictions.shape}")

    # Generate OOF predictions for each fold
    for fold_idx, (train_idx, val_idx) in enumerate(splits):
        logger.info(
            f"Fold {fold_idx + 1}/{k} | train={len(train_idx)} val={len(val_idx)}"
        )

        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        # Split expansion indices for this fold
        if expansion_start is not None and expansion_end is not None:
            exp_start_train, exp_start_val = expansion_start[train_idx], expansion_start[val_idx]
            exp_end_train, exp_end_val = expansion_end[train_idx], expansion_end[val_idx]
        else:
            exp_start_train, exp_start_val = None, None
            exp_end_train, exp_end_val = None, None

        # SMOTE removed per Phase 1c - use controlled augmentation with KS p-value validation instead
        # Apply SMOTE is now DEPRECATED and ignored
        if apply_smote:
            logger.warning(
                f"Fold {fold_idx + 1}: apply_smote=True is DEPRECATED and ignored. "
                "Use controlled augmentation with quality gates (see data/synthetic_cache/) instead."
            )

        # Train model on K-1 folds (possibly augmented)
        # Pass device parameter for deep learning models
        # Extract pre-trained encoder path if present (don't pass to __init__)
        load_pretrained_encoder = model_kwargs.pop("load_pretrained_encoder", None)
        model = get_model(model_name, seed=seed, device=device, **model_kwargs)

        # For SimpleLSTM, we need to build the model first, then load encoder
        if model_name == "simple_lstm" and load_pretrained_encoder:
            # Build model on training data first
            model.fit(X_train, y_train, expansion_start=exp_start_train, expansion_end=exp_end_train)
            # Then load pre-trained encoder and refit with frozen encoder
            logger.info(f"Loading pre-trained encoder from: {load_pretrained_encoder}")
            model.load_pretrained_encoder(load_pretrained_encoder, freeze_encoder=True)
            # Refit with frozen encoder
            unfreeze_epochs = 10  # From config: MASKED_LSTM_UNFREEZE_EPOCHS
            model.fit(X_train, y_train, expansion_start=exp_start_train, expansion_end=exp_end_train, unfreeze_encoder_after=unfreeze_epochs)
        else:
            model.fit(X_train, y_train, expansion_start=exp_start_train, expansion_end=exp_end_train)

        # Predict probabilities on held-out fold
        val_proba = model.predict_proba(X_val, expansion_start=exp_start_val, expansion_end=exp_end_val)

        # Store predictions in OOF matrix
        oof_predictions[val_idx] = val_proba

        # Log fold metrics
        val_pred = model.predict(X_val, expansion_start=exp_start_val, expansion_end=exp_end_val)
        fold_acc = (val_pred == y_val).mean()
        logger.info(f"Fold {fold_idx + 1} accuracy: {fold_acc:.3f}")

    # Verify no zero rows (all samples should have predictions)
    zero_rows = np.where(np.all(oof_predictions == 0, axis=1))[0]
    if len(zero_rows) > 0:
        logger.warning(f"Found {len(zero_rows)} rows with all-zero predictions")
    else:
        logger.info("All samples have non-zero OOF predictions")

    # Compute OOF metrics
    oof_preds_labels = np.argmax(oof_predictions, axis=1)
    unique_labels = np.unique(y)
    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}
    y_indices = np.array([label_to_idx[label] for label in y])

    oof_accuracy = (oof_preds_labels == y_indices).mean()
    logger.info(f"Overall OOF accuracy: {oof_accuracy:.4f}")

    # Compute per-class metrics
    per_class_metrics = {}
    for label_idx, label_name in enumerate(unique_labels):
        mask = y_indices == label_idx
        if mask.sum() > 0:
            class_acc = (oof_preds_labels[mask] == label_idx).mean()
            per_class_metrics[f"accuracy_class_{label_name}"] = class_acc
            logger.info(f"Class '{label_name}' accuracy: {class_acc:.4f}")

    # Create .build.lock during write
    output_path.parent.mkdir(parents=True, exist_ok=True)
    lock_file = output_path.parent / ".build.lock"

    try:
        lock_file.touch()
        logger.info(f"Created .build.lock at {lock_file}")

        # Save OOF predictions
        np.save(output_path, oof_predictions)
        logger.info(f"Saved OOF predictions to {output_path} | shape={oof_predictions.shape}")
    finally:
        # Remove lock file after write completes
        if lock_file.exists():
            lock_file.unlink()
            logger.info(f"Removed .build.lock")

    # MLflow tracking
    if mlflow_tracking and MLFLOW_AVAILABLE:
        try:
            # Set experiment
            mlflow.set_experiment(mlflow_experiment)

            # Start run
            run_name = f"{model_name}_oof_seed{seed}"
            if apply_smote:
                run_name += f"_smote{smote_target_count}"

            with mlflow.start_run(run_name=run_name):
                # Log parameters
                mlflow.log_params({
                    "model": model_name,
                    "seed": seed,
                    "n_folds": k,
                    "n_samples": len(y),
                    "n_classes": n_classes,
                    "device": device,
                    "apply_smote": apply_smote,
                    "smote_target_count": smote_target_count if apply_smote else None,
                })

                # Log model-specific hyperparameters
                for key, value in model_kwargs.items():
                    if isinstance(value, (int, float, str, bool)):
                        mlflow.log_param(f"model_{key}", value)

                # Log metrics
                mlflow.log_metric("oof_accuracy", oof_accuracy)
                for metric_name, metric_value in per_class_metrics.items():
                    mlflow.log_metric(metric_name, metric_value)

                # Log OOF predictions file as artifact
                mlflow.log_artifact(str(output_path))

                logger.info(f"[MLFLOW] Logged experiment to '{mlflow_experiment}' | run='{run_name}'")

        except Exception as e:
            logger.error(f"[MLFLOW] Failed to log experiment: {e}")
    elif mlflow_tracking and not MLFLOW_AVAILABLE:
        logger.warning("[MLFLOW] Tracking requested but MLflow not installed")

    return oof_predictions
